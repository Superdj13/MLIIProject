{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fe0b149-6597-40b5-9dc6-3fc75306716b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from copy import deepcopy\n",
    "from torch import nn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam, SGD, AdamW\n",
    "import numpy as np\n",
    "import torchaudio\n",
    "from IPython.display import Audio\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torchvision.transforms as vtransforms\n",
    "import deepfool\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "008889d1-3dae-4028-bafb-314b3331f54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "directory = \"DIRECTORY HERE\"   ## This must include the parent directory of folds\n",
    "csv_file = np.genfromtxt(\"UrbanSound8K.csv\",delimiter=\",\",dtype=str)  ## csv file must be in the present directory\n",
    "inds = []\n",
    "for i in range(1,11):\n",
    "    inds += [ np.where(csv_file[:,5] == f\"{i}\")[0] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c498f052-595a-4650-bf12-2dfd6b59cd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(i): # /media/carlos/5E48D2B648D28BE1/Users/pedro/Desktop/audio/\n",
    "    file,fold = csv_file[i,0],csv_file[i,5]\n",
    "    return torchaudio.load(directory + f\"fold{fold}/{file}\")\n",
    "\n",
    "## PRE-PROCESS\n",
    "def mono_channel_equal_size(wave,new_freq):\n",
    "    waveform = wave.mean(0)\n",
    "    waveform = np.resize(waveform,4 * new_freq) ## resize to 4 seconds. some do not have 44100 sampling rate and will have less time.\n",
    "    return torch.from_numpy(waveform).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8a737d3-23cb-4798-bfd0-fe2ca2fd3c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fold_Set(Dataset):\n",
    "    def __init__(self,fold, transform=None, shorten_factor = 15):\n",
    "        super().__init__()\n",
    "        self.fold = fold\n",
    "        self.dir = directory+f\"fold{fold}/\"\n",
    "        self.inds_fold = inds[fold-1]\n",
    "        self.new_freq = 44100 // shorten_factor\n",
    "        self.shorten_factor = shorten_factor\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.inds_fold)\n",
    "    def __getitem__(self,i):\n",
    "        label = csv_file[self.inds_fold[i],6]\n",
    "        audio,sample_rate = load_file(self.inds_fold[i])\n",
    "        resampler = torchaudio.transforms.Resample(sample_rate,self.new_freq)\n",
    "        audio_preprocessed = resampler(audio)\n",
    "        audio_preprocessed = mono_channel_equal_size(audio_preprocessed, self.new_freq)\n",
    "        audio_preprocessed /= audio_preprocessed.max()\n",
    "        \n",
    "        spectre = torchaudio.transforms.MelSpectrogram(sample_rate = self.new_freq, \n",
    "                                                       normalized = True, n_fft = 16384 ,win_length = int(400e-3*self.new_freq), \n",
    "                                                       hop_length = int(31.5e-3*self.new_freq), n_mels = 128 ).to(device)\n",
    "        audio_spectro_mel = torch.unsqueeze(spectre(audio_preprocessed), dim= 0)\n",
    "        to_DB = torchaudio.transforms.AmplitudeToDB(top_db = 80)\n",
    "        audio_spectro_mel = to_DB(audio_spectro_mel)\n",
    "        return audio_spectro_mel , int(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42bc3f0b-6216-43a6-aed0-0768238a1908",
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = []\n",
    "for i in range(10):\n",
    "    folds += [Fold_Set(i+1,shorten_factor = 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7883f8a3-47e8-4f00-8b7e-5135ba6f36a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SAVE MEL SPECROGRAMS TO AVOID REDUNDANT RECALCULATIONS\n",
    "for j in range(10):\n",
    "    for i in range(len(folds[j])):\n",
    "        torch.save(folds[j][i],f\"./preprocessed/fold{j+1}/sample{i}.pk\")   ##in the present directory there must be preprocessed/fold1, etc... directories for the 10 folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a9d02f9c-196c-463e-84c9-5886a95071c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fold_PreprocessedSet(Dataset):\n",
    "    def __init__(self,fold,aug = True): ## CHANGE DIR IN THE END\n",
    "        super().__init__()\n",
    "        self.fold = fold\n",
    "        self.dir = lambda i: f\"./preprocessed/fold{fold}/sample{i}.pk\"\n",
    "        self.transform = nn.Sequential(\n",
    "            vtransforms.RandomApply(transforms=[torchaudio.transforms.FrequencyMasking(10),torchaudio.transforms.TimeMasking(30)]),\n",
    "            vtransforms.RandomApply(transforms=[vtransforms.RandomRotation(5),vtransforms.RandomVerticalFlip() ,vtransforms.RandomHorizontalFlip()]),\n",
    "            vtransforms.Normalize(.3,0.4)\n",
    "                                        )\n",
    "        self.transform_test = vtransforms.Normalize(.3,0.4)\n",
    "        self.aug = aug\n",
    "    def __len__(self):\n",
    "        return len(inds[self.fold-1])\n",
    "    def __getitem__(self,i):\n",
    "        sample, label = torch.load(self.dir(i))\n",
    "        if self.aug == False:\n",
    "            return self.transform_test(sample),label\n",
    "        return self.transform(sample),label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "82eb66bb-106e-4dbb-b2c4-a2400004fc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = [] # redefine this list with the proper preprocessed dataset\n",
    "fold_loaders = [] # define the loaders for training\n",
    "val_loaders = [] # loaders for validation\n",
    "test_loaders = [] # loader for testing\n",
    "batch_size = 64\n",
    "def custom_collate(batch):\n",
    "    a,b = torch.utils.data.default_collate(batch)\n",
    "    return a,b.to(device)  ## collate function to use proper device at all times\n",
    "for i in range(10):\n",
    "    folds += [Fold_PreprocessedSet(i+1)]\n",
    "for i in range(10):\n",
    "    full_train = folds[(i+2) % 10]\n",
    "    for j in range(3,10):\n",
    "        full_train += folds[(i+j) % 10]\n",
    "    fold_loaders += [DataLoader(full_train,batch_size = batch_size, shuffle = True, collate_fn = custom_collate)] \n",
    "    folds_i_val = Fold_PreprocessedSet( (i+1) % 10 + 1 , aug = False)  ## no augmentation in val/test\n",
    "    folds_i_test = Fold_PreprocessedSet( i % 10 + 1 , aug = False)\n",
    "    val_loaders += [DataLoader(folds_i_val,batch_size = batch_size, shuffle = True, collate_fn = custom_collate)]\n",
    "    test_loaders += [DataLoader(folds_i_test,batch_size = batch_size, shuffle = True, collate_fn = custom_collate)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2a6ad299-acf2-47f2-ac30-ee06113dff74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self,in_c,out_c,expansion,stride=1,downsample=None):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.downsample = downsample\n",
    "        self.conv1 = nn.Conv2d(in_c,out_c,kernel_size = 3,stride = stride,padding = 1,bias = False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_c)\n",
    "        self.relu = nn.ReLU(inplace = True)\n",
    "        self.conv2 = nn.Conv2d(out_c,out_c*expansion,kernel_size = 3,padding = 1,bias = False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_c*expansion)\n",
    "\n",
    "    def forward(self,x):\n",
    "        Id = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        if self.downsample:\n",
    "            Id = self.downsample(x)\n",
    "    \n",
    "        out += Id\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "        \n",
    "        \n",
    "class ResNet(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        block,\n",
    "        img_channels: int = 1,\n",
    "        num_classes: int  = 10\n",
    "    ) -> None:\n",
    "        super(ResNet, self).__init__()\n",
    "            # The following `layers` list defines the number of `BasicBlock` \n",
    "            # to use to build the network and how many basic blocks to stack\n",
    "            # together.\n",
    "        layers = [5, 10]\n",
    "        self.expansion = 1\n",
    "        \n",
    "        self.in_channels = 64\n",
    "        \n",
    "        self.conv1 = nn.Conv2d( ##(64,134,134)\n",
    "            in_channels=img_channels,\n",
    "            out_channels=self.in_channels,\n",
    "            kernel_size=7, \n",
    "            stride=2,\n",
    "            padding=3,\n",
    "            bias=False\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm2d(self.in_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1) ## (64,68,68)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        # self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        # self.layer4 = self._make_layer(block, 512, layers[3], stride=2) # Disregard ResNet18 architecture\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(128*self.expansion, num_classes)\n",
    "    def _make_layer(\n",
    "        self, \n",
    "        block,\n",
    "        out_channels: int,\n",
    "        blocks: int,\n",
    "        stride: int = 1\n",
    "    ) -> nn.Sequential:\n",
    "        downsample = None\n",
    "        if stride != 1:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    self.in_channels, \n",
    "                    out_channels*self.expansion,\n",
    "                    kernel_size=1,\n",
    "                    stride=stride,\n",
    "                    bias=False \n",
    "                ),\n",
    "                nn.BatchNorm2d(out_channels * self.expansion),\n",
    "            )\n",
    "        layers = []\n",
    "        layers.append(\n",
    "            block(\n",
    "                self.in_channels, out_channels, self.expansion, stride, downsample\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        for i in range(1, blocks):\n",
    "            self.in_channels = out_channels * self.expansion\n",
    "            layers.append(block(\n",
    "                self.in_channels,\n",
    "                out_channels,\n",
    "                expansion=self.expansion\n",
    "            ))\n",
    "        return nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        # x = self.layer3(x)\n",
    "        # x = self.layer4(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "81587742-233b-4b10-a8e7-c78f90cd2b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        self.Bottom = nn.Sequential(\n",
    "            nn.Linear(128*128 , 512, bias = False), ##Normalization nullifies any bias preceding it directly, therefore not needed\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,256, bias = False),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256,128, bias = False),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,128, bias = False),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128,256, bias = False),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256,128, bias = False),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            \n",
    "            nn.Linear(128,10),\n",
    "            nn.ReLU()\n",
    "                            )\n",
    "    def forward(self,x):\n",
    "        x = self.flatten(x)\n",
    "        out = self.Bottom ( x  )\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e4396d3c-daee-40a1-90dc-266fce577b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss(label_smoothing = 5e-3)\n",
    "ConvModel = ResNet(BasicBlock).to(device)\n",
    "MLP_Model = MLP().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "97abf608-883e-4d7b-8a49-21635afb81e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_Conv = AdamW(ConvModel.parameters(),amsgrad=True,lr =1e-5,betas = (.99,.999),weight_decay=1e-1)\n",
    "optimizer_MLP = AdamW(MLP_Model.parameters(),amsgrad=True,lr =1e-5,betas = (.99,.999),weight_decay=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cafbd174-734d-4681-a318-c5f6da924db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_NN(start,loader,loss_fn,optimizer,NNmodel, writer):\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    NNmodel.train() ##Training mode: important for Dropout\n",
    "    size = len(loader.dataset)\n",
    "    losses = []\n",
    "    r_loss = 0\n",
    "    epochs = [start*size]\n",
    "    for i,(x,y) in enumerate(loader): ## will complete an Epoch\n",
    "\n",
    "        # y = y.to(device)\n",
    "        NetForward = NNmodel.forward(x)\n",
    "        loss = loss_fn(NetForward,y)\n",
    "\n",
    "        ## Keep track of training loss\n",
    "        losses += [loss.item()]\n",
    "        epochs += [epochs[i]+len(x)] #divide by size in the end\n",
    "\n",
    "        r_loss += losses[i]\n",
    "        \n",
    "        if ( (i+1) % 5 == 0):\n",
    "            avg_loss, r_loss = r_loss / 5, 0\n",
    "            writer.add_scalar(\"Loss/Train\",avg_loss,epochs[i+1])\n",
    "        \n",
    "        #Gradient descent with optimizer\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"\"\n",
    "    return (losses, epochs[1:])\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "\n",
    "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"pooled\"\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "    losses = []\n",
    "    epochs = [0]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i,(X, y) in enumerate(dataloader):\n",
    "            \n",
    "            pred = model(X)\n",
    "            # y = y.to(device)\n",
    "            loss = loss_fn(pred, y).item()\n",
    "            test_loss += loss\n",
    "\n",
    "            losses += [loss]\n",
    "            epochs += [ epochs[i]+len(X)/size ]\n",
    "            \n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"\" ##HELPS MEMORY ALLOCATION BY CUDA\n",
    "    \n",
    "    correct /= size\n",
    "    print(f\"Accuracy:{correct}, Loss: {test_loss}\")\n",
    "    return test_loss, correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "425e9ec2-5672-44a5-bb7e-3ffbbaa0b318",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conf_mat(test_fold,model):\n",
    "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"pooled\"\n",
    "    model.eval()\n",
    "    dataloader = test_loaders[test_fold]\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    y_pred,y_true = [],[]\n",
    "    with torch.no_grad():\n",
    "        for i,(X, y) in enumerate(dataloader):        \n",
    "            pred = model(X)\n",
    "            pred = pred.argmax(1).cpu().numpy() ##list of preds\n",
    "            # y = y.to(device)\n",
    "            y_pred.extend(pred)\n",
    "            y_true.extend(y.cpu().numpy())\n",
    "    return confusion_matrix(y_true,y_pred)\n",
    "def test_robustness(test_fold,model):\n",
    "    \n",
    "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"pooled\"\n",
    "    dataset = test_loaders[test_fold].dataset\n",
    "    size = len(dataset)\n",
    "    running_robustness = 0.0\n",
    "\n",
    "    for i in range(size):\n",
    "        image,_ = dataset[i]\n",
    "        perturbation,_,_,_,_ = deepfool.deepfool(image[None,:], model)\n",
    "        image_norm = torch.linalg.norm(image)\n",
    "        perturbation_norm = torch.linalg.norm(perturbation)\n",
    "        running_robustness += perturbation_norm / image_norm\n",
    "    return running_robustness/size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f59818f6-6e32-4da8-b3fc-723047bbccef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cross_fold(test_fold,loss,optimizer,model,writer,writer_val):\n",
    "    # test_fold with 0 indexing\n",
    "    n_epochs_target = 12\n",
    "    start_epoch = 0\n",
    "    best_loss = 1e29\n",
    "    \n",
    "    val_size = len(folds[(test_fold +1) % 10])\n",
    "    test_size = len(folds[test_fold])\n",
    "    train_size = 8732 - val_size - test_size   ## total size - val_size - test_size\n",
    "    losses = np.array([])\n",
    "    epochs = np.array([0])\n",
    "    epochs_val = np.array([])\n",
    "    losses_val = np.array([])\n",
    "    \n",
    "    \n",
    "    for i in range(n_epochs_target):\n",
    "        losses_j, epochs_j = train_NN(i+start_epoch,fold_loaders[test_fold],loss,optimizer,model,writer)\n",
    "        epochs = np.append(epochs,  np.array(epochs_j)/train_size )\n",
    "        losses = np.append(losses, np.array(losses_j))\n",
    "        val_loss,acc = test_loop(val_loaders[test_fold],model,loss)\n",
    "        losses_val = np.append( losses_val , val_loss  )\n",
    "        epochs_val = np.append( epochs_val, start_epoch+i+1 )\n",
    "        writer_val.add_scalar(\"Loss/Train\",losses_val[-1],epochs_val[-1]*train_size) ## Use tensorboard, if you want, but you can plot with matplotlib too\n",
    "        if val_loss < best_loss: ##Choose model based on best val accuracy since weight decay is employed\n",
    "            best_acc = acc\n",
    "            best_loss = val_loss\n",
    "            best_model = deepcopy(model)\n",
    "    epochs = epochs[1:]\n",
    "\n",
    "    print(\"TEST\")\n",
    "    test_loss,test_acc = test_loop(test_loaders[test_fold],best_model,loss)\n",
    "    ConfM = conf_mat(test_fold,best_model)\n",
    "\n",
    "    return test_loss,test_acc,best_loss,best_acc,ConfM,best_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "29a807f9-3a74-4395-9a1f-fd124bf68d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.4144144144144144, Loss: 1.7198339785848344\n",
      "Accuracy:0.5213963963963963, Loss: 1.5483811157090324\n",
      "Accuracy:0.5833333333333334, Loss: 1.2985307318823678\n",
      "Accuracy:0.6024774774774775, Loss: 1.222632212298257\n",
      "Accuracy:0.6295045045045045, Loss: 1.194753987448556\n",
      "Accuracy:0.6227477477477478, Loss: 1.208483099937439\n",
      "Accuracy:0.634009009009009, Loss: 1.1604133588927132\n",
      "Accuracy:0.6565315315315315, Loss: 1.146526962518692\n",
      "Accuracy:0.6509009009009009, Loss: 1.061855388539178\n",
      "Accuracy:0.661036036036036, Loss: 1.1493688992091589\n",
      "Accuracy:0.6779279279279279, Loss: 1.059755619083132\n",
      "Accuracy:0.6666666666666666, Loss: 0.9907143328871045\n",
      "TEST\n",
      "Accuracy:0.6781214203894617, Loss: 1.0441330543586187\n",
      "Accuracy:0.3567567567567568, Loss: 1.970622984568278\n",
      "Accuracy:0.44432432432432434, Loss: 1.6019457976023357\n",
      "Accuracy:0.5589189189189189, Loss: 1.3396233161290487\n",
      "Accuracy:0.5989189189189189, Loss: 1.1112003763516745\n",
      "Accuracy:0.5913513513513513, Loss: 1.1829696377118428\n",
      "Accuracy:0.6432432432432432, Loss: 1.0807234168052673\n",
      "Accuracy:0.6281081081081081, Loss: 1.0409667253494264\n",
      "Accuracy:0.6367567567567568, Loss: 1.0820931156476339\n",
      "Accuracy:0.7037837837837838, Loss: 0.8663056890169779\n",
      "Accuracy:0.6378378378378379, Loss: 1.0731337428092957\n",
      "Accuracy:0.6378378378378379, Loss: 1.0946306069691976\n",
      "Accuracy:0.6313513513513513, Loss: 1.161834748586019\n",
      "TEST\n",
      "Accuracy:0.6148648648648649, Loss: 1.141483954020909\n",
      "Accuracy:0.2909090909090909, Loss: 1.9653293564915657\n",
      "Accuracy:0.45555555555555555, Loss: 1.551416553556919\n",
      "Accuracy:0.5121212121212121, Loss: 1.3335012719035149\n",
      "Accuracy:0.5717171717171717, Loss: 1.2054244428873062\n",
      "Accuracy:0.5767676767676768, Loss: 1.1634772457182407\n",
      "Accuracy:0.5888888888888889, Loss: 1.1657008230686188\n",
      "Accuracy:0.6323232323232323, Loss: 1.1121744401752949\n",
      "Accuracy:0.6191919191919192, Loss: 1.1107322722673416\n",
      "Accuracy:0.6303030303030303, Loss: 1.118535939604044\n",
      "Accuracy:0.6515151515151515, Loss: 1.0776275768876076\n",
      "Accuracy:0.6575757575757576, Loss: 1.1100978925824165\n",
      "Accuracy:0.6434343434343435, Loss: 1.1192464753985405\n",
      "TEST\n",
      "Accuracy:0.7178378378378378, Loss: 0.9315481225649516\n",
      "Accuracy:0.34615384615384615, Loss: 1.8959671179453532\n",
      "Accuracy:0.5117521367521367, Loss: 1.4177263895670573\n",
      "Accuracy:0.6698717948717948, Loss: 1.1630061626434327\n",
      "Accuracy:0.6805555555555556, Loss: 0.9366792758305867\n",
      "Accuracy:0.7735042735042735, Loss: 0.7728811740875244\n",
      "Accuracy:0.7617521367521367, Loss: 0.7448739171028137\n",
      "Accuracy:0.7938034188034188, Loss: 0.6986763079961141\n",
      "Accuracy:0.7788461538461539, Loss: 0.7452016433080038\n",
      "Accuracy:0.7777777777777778, Loss: 0.6894874056180318\n",
      "Accuracy:0.7884615384615384, Loss: 0.6477173447608948\n",
      "Accuracy:0.7788461538461539, Loss: 0.697086751461029\n",
      "Accuracy:0.7713675213675214, Loss: 0.7013096690177918\n",
      "TEST\n",
      "Accuracy:0.6373737373737374, Loss: 1.2357711344957352\n",
      "Accuracy:0.30984204131227217, Loss: 1.9117583128122182\n",
      "Accuracy:0.4726609963547995, Loss: 1.5631283521652222\n",
      "Accuracy:0.5479951397326853, Loss: 1.4339567514566274\n",
      "Accuracy:0.5929526123936817, Loss: 1.309644368978647\n",
      "Accuracy:0.5941676792223572, Loss: 1.2354968190193176\n",
      "Accuracy:0.6111786148238153, Loss: 1.1735915954296405\n",
      "Accuracy:0.6221142162818954, Loss: 1.1768073118649995\n",
      "Accuracy:0.6281895504252734, Loss: 1.142580903493441\n",
      "Accuracy:0.6354799513973268, Loss: 1.1831875901955824\n",
      "Accuracy:0.6415552855407047, Loss: 1.1589768620637746\n",
      "Accuracy:0.6354799513973268, Loss: 1.2109616215412433\n",
      "Accuracy:0.6330498177399757, Loss: 1.1719505144999578\n",
      "TEST\n",
      "Accuracy:0.7158119658119658, Loss: 0.882418672243754\n",
      "Accuracy:0.4021479713603819, Loss: 1.7894629069737025\n",
      "Accuracy:0.45584725536992843, Loss: 1.5805148822920663\n",
      "Accuracy:0.49522673031026254, Loss: 1.427145745073046\n",
      "Accuracy:0.5584725536992841, Loss: 1.2549099028110504\n",
      "Accuracy:0.6026252983293556, Loss: 1.171718520777566\n",
      "Accuracy:0.6097852028639618, Loss: 1.0783299037388392\n",
      "Accuracy:0.6467780429594272, Loss: 1.0691649871213096\n",
      "Accuracy:0.6575178997613366, Loss: 1.0855568732534135\n",
      "Accuracy:0.6933174224343676, Loss: 1.155592484133584\n",
      "Accuracy:0.6921241050119332, Loss: 1.146776169538498\n",
      "Accuracy:0.6718377088305489, Loss: 1.0636617541313171\n",
      "Accuracy:0.6754176610978521, Loss: 1.173743405512401\n",
      "TEST\n",
      "Accuracy:0.707168894289186, Loss: 0.9841025379987863\n",
      "Accuracy:0.38461538461538464, Loss: 1.7500870778010442\n",
      "Accuracy:0.5471464019851117, Loss: 1.4594284846232488\n",
      "Accuracy:0.5918114143920595, Loss: 1.3045649987000685\n",
      "Accuracy:0.6166253101736973, Loss: 1.2732025476602407\n",
      "Accuracy:0.6612903225806451, Loss: 1.151267253435575\n",
      "Accuracy:0.6563275434243176, Loss: 1.1829135005290692\n",
      "Accuracy:0.641439205955335, Loss: 1.2245012705142682\n",
      "Accuracy:0.598014888337469, Loss: 1.206056026312021\n",
      "Accuracy:0.6799007444168734, Loss: 1.1215285750535817\n",
      "Accuracy:0.6265508684863523, Loss: 1.2247193960043101\n",
      "Accuracy:0.6699751861042184, Loss: 1.2262567648520837\n",
      "Accuracy:0.607940446650124, Loss: 1.2608964626605694\n",
      "TEST\n",
      "Accuracy:0.6670644391408115, Loss: 1.1711115624223436\n",
      "Accuracy:0.38357843137254904, Loss: 1.779805091711191\n",
      "Accuracy:0.44730392156862747, Loss: 1.449723060314472\n",
      "Accuracy:0.5588235294117647, Loss: 1.2720966614209688\n",
      "Accuracy:0.6556372549019608, Loss: 1.0777890177873464\n",
      "Accuracy:0.6299019607843137, Loss: 0.984206130871406\n",
      "Accuracy:0.7046568627450981, Loss: 0.9056778366749103\n",
      "Accuracy:0.7009803921568627, Loss: 0.9618137295429523\n",
      "Accuracy:0.7230392156862745, Loss: 0.8433430561652551\n",
      "Accuracy:0.7107843137254902, Loss: 0.9092400394953214\n",
      "Accuracy:0.7475490196078431, Loss: 0.8608287802109351\n",
      "Accuracy:0.7279411764705882, Loss: 0.867801872583536\n",
      "Accuracy:0.7340686274509803, Loss: 0.9717029058016263\n",
      "TEST\n",
      "Accuracy:0.6811414392059554, Loss: 1.2056353321442237\n",
      "Accuracy:0.44922341696535245, Loss: 1.53770398242133\n",
      "Accuracy:0.5197132616487455, Loss: 1.2840049948011125\n",
      "Accuracy:0.5949820788530465, Loss: 1.147565130676542\n",
      "Accuracy:0.6666666666666666, Loss: 1.0300379182611192\n",
      "Accuracy:0.6881720430107527, Loss: 1.016906350851059\n",
      "Accuracy:0.6774193548387096, Loss: 1.0093208593981606\n",
      "Accuracy:0.6965352449223416, Loss: 0.9349251134055001\n",
      "Accuracy:0.7335722819593787, Loss: 0.8393490889242717\n",
      "Accuracy:0.7419354838709677, Loss: 0.8358090264456612\n",
      "Accuracy:0.7228195937873357, Loss: 0.8392402103969029\n",
      "Accuracy:0.7526881720430108, Loss: 0.8064560230289187\n",
      "Accuracy:0.7502986857825568, Loss: 0.7469062656164169\n",
      "TEST\n",
      "Accuracy:0.7071078431372549, Loss: 0.9194666972527137\n",
      "Accuracy:0.5246277205040092, Loss: 1.4098908049719674\n",
      "Accuracy:0.5910652920962199, Loss: 1.2630138312067305\n",
      "Accuracy:0.6380297823596792, Loss: 1.1284743334565843\n",
      "Accuracy:0.6827033218785796, Loss: 1.0116016864776611\n",
      "Accuracy:0.7033218785796106, Loss: 0.8901768582207816\n",
      "Accuracy:0.6987399770904925, Loss: 0.8562911152839661\n",
      "Accuracy:0.7296678121420389, Loss: 0.8395438109125409\n",
      "Accuracy:0.6758304696449027, Loss: 0.9003735695566449\n",
      "Accuracy:0.7044673539518901, Loss: 0.917572694165366\n",
      "Accuracy:0.6712485681557846, Loss: 0.9499185170446124\n",
      "Accuracy:0.6827033218785796, Loss: 0.9029290335518974\n",
      "Accuracy:0.695303550973654, Loss: 0.8845898083278111\n",
      "TEST\n",
      "Accuracy:0.7347670250896058, Loss: 0.8337450282914298\n"
     ]
    }
   ],
   "source": [
    "## CROSS FOLD VALIDATION FOR RESNET\n",
    "val_loss_avg = 0\n",
    "val_acc_avg = 0\n",
    "test_loss_avg = 0\n",
    "test_accs = torch.zeros(10)\n",
    "rob_avg = 0\n",
    "ConfM_avg = np.zeros((10,10),dtype = \"float32\")\n",
    "for i in range(10):\n",
    "    ConvModel = ResNet(BasicBlock).to(device)\n",
    "    optimizer_Conv = AdamW(ConvModel.parameters(),amsgrad=True,lr =1e-5,betas = (.99,.999),weight_decay=1e-1)\n",
    "    \n",
    "    writer = SummaryWriter(f'runs/treinoConv_fold{i+1}')\n",
    "    writer_val = SummaryWriter(f'runs/valConv_fold{i+1}')\n",
    "    test_loss,test_acc,val_loss,val_acc,ConfM,best_model = train_cross_fold(i,loss,optimizer_Conv,ConvModel,writer,writer_val)\n",
    "    \n",
    "    rob_avg += test_robustness(i,best_model)\n",
    "    ConfM_avg += ConfM\n",
    "    test_loss_avg += test_loss\n",
    "    val_loss_avg += val_loss\n",
    "    test_accs[i] = test_acc\n",
    "    val_acc_avg += val_acc\n",
    "\n",
    "test_loss_avg /= 10\n",
    "val_loss_avg /= 10\n",
    "test_acc_avg = test_accs.mean()\n",
    "test_acc_std = test_accs.std()\n",
    "val_acc_avg /= 10\n",
    "ConfM_avg /= 10\n",
    "rob_avg /= 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3ccff397-ad7b-480c-ae7d-9ac5de16ea84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet Test Loss: 1.0349416095793467\n",
      "ResNet Test Accuracy: 0.6861258745193481 with std of 0.03806876018643379\n",
      "ResNet Validation Loss: 0.9339929308925138\n",
      "ResNet Validation Accuracy: 0.6993360857710706\n",
      "ResNet Test Robustness: 0.004365378059446812\n",
      "ResNet Test Confusion Matrix:\n",
      " [[49.9  1.5  4.3  7.3  6.  12.9  0.1 12.1  2.6  3.3]\n",
      " [ 1.4 28.9  0.3  0.3  2.9  0.1  0.   1.1  1.2  6.7]\n",
      " [ 1.3  0.  77.9  8.3  1.3  2.3  1.   1.1  2.7  4.1]\n",
      " [ 3.2  0.2  8.5 76.8  2.   1.4  1.8  0.2  4.5  1.4]\n",
      " [ 3.1  0.5  1.7  1.6 69.4  2.5  1.3 15.5  3.5  0.9]\n",
      " [20.6  0.3  3.6  0.6  6.5 55.6  0.1  9.8  2.   0.9]\n",
      " [ 0.3  0.   0.4  1.4  0.7  0.3 33.5  0.5  0.1  0.2]\n",
      " [ 9.3  0.   1.   0.  14.4 11.3  0.1 63.5  0.3  0.1]\n",
      " [ 1.9  3.3  8.2  3.1  3.9  1.6  0.   0.4 65.1  5.4]\n",
      " [ 4.6  4.3  7.2  1.3  2.2  0.4  0.1  1.1  0.9 77.9]]\n"
     ]
    }
   ],
   "source": [
    "## RESNET RESULTS\n",
    "print(f\"ResNet Test Loss: {test_loss_avg}\")\n",
    "print(f\"ResNet Test Accuracy: {test_acc_avg} with std of {test_acc_std}\")\n",
    "print(f\"ResNet Validation Loss: {val_loss_avg}\")\n",
    "print(f\"ResNet Validation Accuracy: {val_acc_avg}\")\n",
    "print(f\"ResNet Test Robustness: {rob_avg}\")\n",
    "print(f\"ResNet Test Confusion Matrix:\\n {ConfM_avg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4ae75d88-646e-47ca-b2a7-93e7a85dfb42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.35135135135135137, Loss: 2.0478422897202626\n",
      "Accuracy:0.4189189189189189, Loss: 1.9306338429450989\n",
      "Accuracy:0.44481981981981983, Loss: 1.8684255906513758\n",
      "Accuracy:0.4527027027027027, Loss: 1.8221396889005388\n",
      "Accuracy:0.45495495495495497, Loss: 1.8110312904630388\n",
      "Accuracy:0.45382882882882886, Loss: 1.7867813791547502\n",
      "Accuracy:0.4752252252252252, Loss: 1.739909793649401\n",
      "Accuracy:0.48536036036036034, Loss: 1.7082099063055856\n",
      "Accuracy:0.48761261261261263, Loss: 1.7115002274513245\n",
      "Accuracy:0.48986486486486486, Loss: 1.6950820173536028\n",
      "Accuracy:0.49211711711711714, Loss: 1.668157500880105\n",
      "Accuracy:0.4797297297297297, Loss: 1.6485908712659563\n",
      "TEST\n",
      "Accuracy:0.5223367697594502, Loss: 1.6377319608415877\n",
      "Accuracy:0.39135135135135135, Loss: 2.0149319728215533\n",
      "Accuracy:0.45297297297297295, Loss: 1.893131637573242\n",
      "Accuracy:0.4821621621621622, Loss: 1.7832216421763103\n",
      "Accuracy:0.4735135135135135, Loss: 1.7129541556040446\n",
      "Accuracy:0.4627027027027027, Loss: 1.7178573290506998\n",
      "Accuracy:0.4572972972972973, Loss: 1.6838759819666544\n",
      "Accuracy:0.4908108108108108, Loss: 1.6246976057688396\n",
      "Accuracy:0.4745945945945946, Loss: 1.6177727619806925\n",
      "Accuracy:0.5037837837837837, Loss: 1.5794770479202271\n",
      "Accuracy:0.5048648648648648, Loss: 1.5581758817036946\n",
      "Accuracy:0.5081081081081081, Loss: 1.5651034832000732\n",
      "Accuracy:0.5113513513513513, Loss: 1.5225179195404053\n",
      "TEST\n",
      "Accuracy:0.5, Loss: 1.6825828552246094\n",
      "Accuracy:0.33535353535353535, Loss: 2.0795892998576164\n",
      "Accuracy:0.38585858585858585, Loss: 1.9304212555289268\n",
      "Accuracy:0.4171717171717172, Loss: 1.8528809696435928\n",
      "Accuracy:0.4444444444444444, Loss: 1.7643452137708664\n",
      "Accuracy:0.45656565656565656, Loss: 1.7173707857728004\n",
      "Accuracy:0.47474747474747475, Loss: 1.6693880036473274\n",
      "Accuracy:0.4717171717171717, Loss: 1.6300640851259232\n",
      "Accuracy:0.4808080808080808, Loss: 1.6439170837402344\n",
      "Accuracy:0.501010101010101, Loss: 1.576805628836155\n",
      "Accuracy:0.5292929292929293, Loss: 1.5541136413812637\n",
      "Accuracy:0.5171717171717172, Loss: 1.5469885841012\n",
      "Accuracy:0.5353535353535354, Loss: 1.5240741893649101\n",
      "TEST\n",
      "Accuracy:0.5372972972972972, Loss: 1.5424529790878296\n",
      "Accuracy:0.31303418803418803, Loss: 2.0690433661142986\n",
      "Accuracy:0.33760683760683763, Loss: 1.966919763882955\n",
      "Accuracy:0.40705128205128205, Loss: 1.8497132698694865\n",
      "Accuracy:0.4391025641025641, Loss: 1.7760130643844605\n",
      "Accuracy:0.4700854700854701, Loss: 1.7343974749247233\n",
      "Accuracy:0.49145299145299143, Loss: 1.687084722518921\n",
      "Accuracy:0.4893162393162393, Loss: 1.6459960460662841\n",
      "Accuracy:0.5213675213675214, Loss: 1.5788192828496297\n",
      "Accuracy:0.5181623931623932, Loss: 1.5655019680658977\n",
      "Accuracy:0.5470085470085471, Loss: 1.495338741938273\n",
      "Accuracy:0.5470085470085471, Loss: 1.5149576981862387\n",
      "Accuracy:0.5790598290598291, Loss: 1.4597223997116089\n",
      "TEST\n",
      "Accuracy:0.5414141414141415, Loss: 1.505919225513935\n",
      "Accuracy:0.31470230862697446, Loss: 2.0582954791875987\n",
      "Accuracy:0.39125151883353587, Loss: 1.9633393379358144\n",
      "Accuracy:0.39489671931956255, Loss: 1.90252790084252\n",
      "Accuracy:0.3973268529769137, Loss: 1.8596756274883564\n",
      "Accuracy:0.40583232077764275, Loss: 1.8328651098104625\n",
      "Accuracy:0.4106925880923451, Loss: 1.8044471740722656\n",
      "Accuracy:0.40704738760631837, Loss: 1.7833302571223333\n",
      "Accuracy:0.41798298906439857, Loss: 1.7716968793135424\n",
      "Accuracy:0.4362089914945322, Loss: 1.7445747302128718\n",
      "Accuracy:0.44106925880923453, Loss: 1.7236962960316584\n",
      "Accuracy:0.4459295261239368, Loss: 1.7076713763750517\n",
      "Accuracy:0.4617253948967193, Loss: 1.6897235925381\n",
      "TEST\n",
      "Accuracy:0.5961538461538461, Loss: 1.4364820718765259\n",
      "Accuracy:0.3054892601431981, Loss: 2.0833255563463484\n",
      "Accuracy:0.360381861575179, Loss: 1.9688364011900765\n",
      "Accuracy:0.38066825775656327, Loss: 1.9092055218560355\n",
      "Accuracy:0.42482100238663484, Loss: 1.8563161066600256\n",
      "Accuracy:0.43675417661097854, Loss: 1.8035141910825456\n",
      "Accuracy:0.4498806682577566, Loss: 1.7635630794933863\n",
      "Accuracy:0.4594272076372315, Loss: 1.7467602831976754\n",
      "Accuracy:0.4021479713603819, Loss: 1.7027027096067155\n",
      "Accuracy:0.46539379474940334, Loss: 1.7017542464392525\n",
      "Accuracy:0.46420047732696895, Loss: 1.6723406910896301\n",
      "Accuracy:0.41766109785202865, Loss: 1.6555085011890955\n",
      "Accuracy:0.5071599045346062, Loss: 1.6688167112214225\n",
      "TEST\n",
      "Accuracy:0.41676792223572295, Loss: 1.7813918957343469\n",
      "Accuracy:0.42431761786600497, Loss: 1.9824472665786743\n",
      "Accuracy:0.4491315136476427, Loss: 1.8588778697527373\n",
      "Accuracy:0.4317617866004963, Loss: 1.776494521361131\n",
      "Accuracy:0.4441687344913151, Loss: 1.7259406034763043\n",
      "Accuracy:0.45533498759305213, Loss: 1.7072460101200984\n",
      "Accuracy:0.46898263027295284, Loss: 1.6631973339961126\n",
      "Accuracy:0.48883374689826303, Loss: 1.636504521736732\n",
      "Accuracy:0.5223325062034739, Loss: 1.6280323083584125\n",
      "Accuracy:0.49007444168734493, Loss: 1.6554373044234056\n",
      "Accuracy:0.5210918114143921, Loss: 1.61459834759052\n",
      "Accuracy:0.4975186104218362, Loss: 1.5924308758515577\n",
      "Accuracy:0.456575682382134, Loss: 1.6165178188910851\n",
      "TEST\n",
      "Accuracy:0.4809069212410501, Loss: 1.6820367574691772\n",
      "Accuracy:0.2965686274509804, Loss: 2.0854730055882382\n",
      "Accuracy:0.36151960784313725, Loss: 1.9410716111843402\n",
      "Accuracy:0.45098039215686275, Loss: 1.8373845723959117\n",
      "Accuracy:0.5257352941176471, Loss: 1.731510299902696\n",
      "Accuracy:0.5379901960784313, Loss: 1.6875813924349272\n",
      "Accuracy:0.553921568627451, Loss: 1.6448974792773907\n",
      "Accuracy:0.5894607843137255, Loss: 1.597621853534992\n",
      "Accuracy:0.5894607843137255, Loss: 1.546496262917152\n",
      "Accuracy:0.6017156862745098, Loss: 1.5081639473254864\n",
      "Accuracy:0.5968137254901961, Loss: 1.4846769663003774\n",
      "Accuracy:0.6017156862745098, Loss: 1.4608849103634174\n",
      "Accuracy:0.6115196078431373, Loss: 1.4027845309330866\n",
      "TEST\n",
      "Accuracy:0.5620347394540943, Loss: 1.4826574325561523\n",
      "Accuracy:0.35603345280764637, Loss: 2.0174295135906766\n",
      "Accuracy:0.4133811230585424, Loss: 1.9263570053236825\n",
      "Accuracy:0.4886499402628435, Loss: 1.829683814729963\n",
      "Accuracy:0.5005973715651135, Loss: 1.7818634510040283\n",
      "Accuracy:0.5197132616487455, Loss: 1.7001993485859461\n",
      "Accuracy:0.5292712066905615, Loss: 1.6897044948169164\n",
      "Accuracy:0.5113500597371565, Loss: 1.6726635949952262\n",
      "Accuracy:0.5352449223416965, Loss: 1.6053460921560014\n",
      "Accuracy:0.5340501792114696, Loss: 1.5740769846098763\n",
      "Accuracy:0.5651135005973715, Loss: 1.5108725258282252\n",
      "Accuracy:0.5890083632019116, Loss: 1.4690090673310416\n",
      "Accuracy:0.5352449223416965, Loss: 1.5231345381055559\n",
      "TEST\n",
      "Accuracy:0.5392156862745098, Loss: 1.5552906164756188\n",
      "Accuracy:0.26575028636884307, Loss: 2.0975591966084073\n",
      "Accuracy:0.38029782359679265, Loss: 1.9147713354655675\n",
      "Accuracy:0.4020618556701031, Loss: 1.8370488030569894\n",
      "Accuracy:0.42955326460481097, Loss: 1.7958879726273673\n",
      "Accuracy:0.424971363115693, Loss: 1.7528428094727653\n",
      "Accuracy:0.49713631156930127, Loss: 1.672153753893716\n",
      "Accuracy:0.4879725085910653, Loss: 1.6656318392072404\n",
      "Accuracy:0.4925544100801833, Loss: 1.6397584591593062\n",
      "Accuracy:0.5028636884306987, Loss: 1.6309552788734436\n",
      "Accuracy:0.4936998854524628, Loss: 1.6431127190589905\n",
      "Accuracy:0.5074455899198167, Loss: 1.6260379127093725\n",
      "Accuracy:0.5257731958762887, Loss: 1.6033213223729814\n",
      "TEST\n",
      "Accuracy:0.5854241338112306, Loss: 1.4402187381471907\n"
     ]
    }
   ],
   "source": [
    "## CROSS FOLD VALIDATION FOR MLP\n",
    "val_loss_avg = 0\n",
    "val_acc_avg = 0\n",
    "test_loss_avg = 0\n",
    "test_accs = torch.zeros(10)\n",
    "ConfM_avg = np.zeros((10,10),dtype = \"float32\")\n",
    "for i in range(10):\n",
    "    MLP_Model = MLP().to(device)\n",
    "    optimizer_MLP = AdamW(MLP_Model.parameters(),amsgrad=True,lr =1e-5,betas = (.99,.999),weight_decay=1e-1)\n",
    "    \n",
    "    writer = SummaryWriter(f'runs/treinoMLP_fold{i+1}')\n",
    "    writer_val = SummaryWriter(f'runs/valMLP_fold{i+1}')\n",
    "    test_loss,test_acc,val_loss,val_acc,ConfM,best_model = train_cross_fold(i,loss,optimizer_MLP,MLP_Model,writer,writer_val)\n",
    "    \n",
    "    rob_avg += test_robustness(i,best_model)\n",
    "    ConfM_avg += ConfM\n",
    "    test_loss_avg += test_loss\n",
    "    val_loss_avg += val_loss\n",
    "    test_accs[i] = test_acc\n",
    "    val_acc_avg += val_acc\n",
    "\n",
    "test_loss_avg /= 10\n",
    "val_loss_avg /= 10\n",
    "test_acc_avg = test_accs.mean()\n",
    "test_acc_std = test_accs.std()\n",
    "val_acc_avg /= 10\n",
    "ConfM_avg /= 10\n",
    "rob_avg /= 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3718cc40-fe11-4466-a45e-0619916453df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Test Loss: 1.5746764532926973\n",
      "MLP Test Accuracy: 0.5281551480293274 with std of 0.052598241716623306\n",
      "MLP Validation Loss: 1.5567683270098742\n",
      "MLP Validation Accuracy: 0.5208700715586367\n",
      "ResNet Test Robustness: 0.013501264154911041\n",
      "MLP Test Confusion Matrix:\n",
      " [[43.6  0.5  4.   3.9  6.9 13.7  1.1  8.7  6.5 11.1]\n",
      " [ 1.9 26.4  2.9  1.3  1.1  1.2  0.1  4.1  1.2  2.7]\n",
      " [ 9.   1.  45.6 10.9  2.5  7.3  1.8  3.2  5.  13.7]\n",
      " [ 4.5  1.2 11.  61.4  1.6  2.6  2.1  0.7  7.2  7.7]\n",
      " [ 4.7  2.3  9.1  3.2 43.5  5.   0.2 21.2  5.6  5.2]\n",
      " [11.2  0.3  9.1  0.5  2.4 54.7  0.1  7.1  8.5  6.1]\n",
      " [ 0.8  0.   2.9  2.3  0.7  0.9 25.9  1.2  0.5  2.2]\n",
      " [10.3  0.3  3.5  0.3 17.6  2.2  0.  45.   9.5 11.3]\n",
      " [ 4.7  0.2  7.6  7.   0.5  2.9  0.   2.6 65.7  1.7]\n",
      " [10.   0.8 14.3  5.7  3.9  4.5  0.1  5.6  4.9 50.2]]\n"
     ]
    }
   ],
   "source": [
    "## MLP RESULTS\n",
    "print(f\"MLP Test Loss: {test_loss_avg}\")\n",
    "print(f\"MLP Test Accuracy: {test_acc_avg} with std of {test_acc_std}\")\n",
    "print(f\"MLP Validation Loss: {val_loss_avg}\")\n",
    "print(f\"MLP Validation Accuracy: {val_acc_avg}\")\n",
    "print(f\"ResNet Test Robustness: {rob_avg}\")\n",
    "print(f\"MLP Test Confusion Matrix:\\n {ConfM_avg}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
